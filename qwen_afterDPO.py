import pandas as pd
import os
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch.nn.functional as F
from scipy.stats import spearmanr, pearsonr
from tqdm import tqdm

# ================= 1. é…ç½®è·¯å¾„ =================
from paths import MODEL_PATH, LORA_DPO_PATH, FINAL_TEST_IMG_DIR, FINAL_TEST_CSV_PATH, HF_HOME

os.environ.setdefault("HF_HOME", HF_HOME)

MODEL_PATH = MODEL_PATH
# âš ï¸ è¿™é‡Œå¿…é¡»æŒ‡å‘ä½ åˆšè·‘å®Œçš„ DPO æ–‡ä»¶å¤¹
LORA_PATH = LORA_DPO_PATH 

IMG_DIR = FINAL_TEST_IMG_DIR
CSV_PATH = FINAL_TEST_CSV_PATH
BATCH_SIZE = 10 # åŒ 3090 å»ºè®® 10-12
MAX_PIXELS = 301056

def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å¹¶æ³¨å…¥ DPO ç»ˆææƒé‡...")
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        MODEL_PATH, torch_dtype=torch.bfloat16, device_map="auto"
    )
    # åŠ è½½ DPO é€‚é…å™¨
    model.load_adapter(LORA_PATH) 
    
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    processor.tokenizer.padding_side = 'left' 

    # å‡†å¤‡ 1-10 çš„ Token ID å’Œæƒé‡
    target_tokens = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]
    token_ids = [processor.tokenizer.encode(t, add_special_tokens=False)[-1] for t in target_tokens]
    weights = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32).to(model.device)

    # è¯»å– CSV
    df = pd.read_csv(CSV_PATH)

    # === ã€ä¿®å¤ 1ã€‘æ›´ç¨³å¥çš„ ID å¤„ç†å‡½æ•° ===
    def clean_id(x):
        try:
            # å°è¯•å¤„ç†åƒ "1001.0" è¿™æ ·çš„çº¯æ•°å­— IDï¼Œè½¬ä¸º "1001"
            return str(int(float(x)))
        except (ValueError, TypeError):
            # å¦‚æœæŠ¥é”™ï¼ˆè¯´æ˜æ˜¯æ–‡ä»¶åå­—ç¬¦ä¸²ï¼‰ï¼Œç›´æ¥å»é™¤ç©ºæ ¼ä¿ç•™åŸæ ·
            return str(x).strip()

    df['ID'] = df['ID'].apply(clean_id)
    # ===================================
    
    results = []
    print(f"ğŸ“Š å¼€å§‹æœ€ç»ˆå¤§è€ƒ (æ ·æœ¬æ•°: {len(df)})...")

    for i in tqdm(range(0, len(df), BATCH_SIZE)):
        batch_df = df.iloc[i : i + BATCH_SIZE]
        batch_msgs, batch_gt = [], []

        for _, row in batch_df.iterrows():
            # === ã€ä¿®å¤ 2ã€‘æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦åŠ  .jpg åç¼€ ===
            img_name = row['ID']
            # å¦‚æœ ID ç»“å°¾ä¸æ˜¯å¸¸è§çš„å›¾ç‰‡åç¼€ï¼Œåˆ™æ‰‹åŠ¨æ·»åŠ  .jpg
            if not img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.webp')):
                img_name = f"{img_name}.jpg"
            
            img_path = os.path.join(IMG_DIR, img_name)
            # ==========================================

            if os.path.exists(img_path):
                # âš ï¸ è¿˜åŸè®­ç»ƒæ—¶çš„ CoT Prompt é¡ºåº
                msg = [{"role": "user", "content": [
                    {"type": "image", "image": img_path, "max_pixels": MAX_PIXELS},
                    {"type": "text", "text": "Analyze this image's aesthetics. Briefly describe the composition and lighting, then provide a rating level from 1 to 10. Format: Analysis: [text] Rating Level: [score]"}
                ]}]
                batch_msgs.append(msg)
                batch_gt.append(row['score'])
            else:
                print(f"âš ï¸ Warning: Image not found: {img_path}")

        if not batch_msgs: continue

        texts = [processor.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in batch_msgs]
        image_inputs, _ = process_vision_info(batch_msgs)
        inputs = processor(text=texts, images=image_inputs, padding=True, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150, # ç»™è¶³å¤Ÿçš„é•¿åº¦å†™ Analysis
                do_sample=False,
                return_dict_in_generate=True,
                output_scores=True
            )

        for b in range(len(batch_msgs)):
            # 1. è·å–ç”Ÿæˆçš„ token åºåˆ—
            gen_ids = outputs.sequences[b][len(inputs.input_ids[b]):]
            
            # 2. ç²¾å‡†å®šä½ï¼šå¯»æ‰¾ç¬¬ä¸€ä¸ªå‡ºç°åœ¨ Analysis ä¹‹åçš„æ•°å­— Token
            rating_token_pos = -1
            for pos, tid in enumerate(gen_ids):
                # è¿™é‡Œçš„ 20 æ˜¯ä¸ºäº†è·³è¿‡å¼€å¤´å¯èƒ½å‡ºç°çš„æ— å…³æ•°å­—ï¼Œç¡®ä¿å–åˆ°çš„æ˜¯ Rating é™„è¿‘çš„
                if tid.item() in token_ids and pos > 20: 
                    rating_token_pos = pos
                    break
            
            if rating_token_pos != -1:
                # 3. æå–è¯¥ä½ç½®çš„ Logits è¿›è¡Œæ¦‚ç‡åŠ æƒ
                logits = outputs.scores[rating_token_pos][b]
                relevant_logits = logits[token_ids]
                
                # åº”ç”¨ Temperature è¿›è¡Œç¼©æ”¾
                temperature = 0.5
                probs = F.softmax(relevant_logits.float() / temperature, dim=-1)
                
                final_score = torch.sum(probs * weights).item()
            else:
                final_score = 5.0 # å…œåº•åˆ†æ•°

            results.append({"gt": batch_gt[b], "pred": final_score})

    # è®¡ç®—æœ€ç»ˆç¡¬æŒ‡æ ‡
    if len(results) > 0:
        res_df = pd.DataFrame(results)
        srcc, _ = spearmanr(res_df['gt'], res_df['pred'])
        plcc, _ = pearsonr(res_df['gt'], res_df['pred'])
        
        print("\n" + "="*50)
        print(f"ğŸ† DPO æœ€ç»ˆè·‘åˆ†ç»“æœ:")
        print(f"SRCC: {srcc:.4f}")
        print(f"PLCC: {plcc:.4f}")
        print(f"Q-Align åŸºå‡†: 0.6557")
        print("="*50)
    else:
        print("\nâŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ‰æ•ˆç»“æœï¼Œè¯·æ£€æŸ¥å›¾ç‰‡è·¯å¾„æˆ– CSV IDã€‚")

if __name__ == "__main__":
    main()