import os
import torch
import uvicorn
import io
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch.nn.functional as F
from PIL import Image
from paths import HF_HOME, MODEL_PATH, LORA_DPO_PATH

# ================= 1. é…ç½®è·¯å¾„ =================
CACHE_DIR = HF_HOME
os.environ.setdefault("HF_HOME", CACHE_DIR)

MODEL_PATH = MODEL_PATH
LORA_PATH = LORA_DPO_PATH
MAX_PIXELS = 301056

# ================= 2. åˆå§‹åŒ– API =================
app = FastAPI(
    title="Qwen2-VL Aesthetic DPO API",
    description="æä¾›ç¾å­¦è¯„åˆ†ã€åˆ†æåŠè¯¦ç»†çš„Logitsæ¦‚ç‡åˆ†å¸ƒ",
    version="1.1"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
model = None
processor = None
token_ids = []
weights = None
# å®šä¹‰è¯„åˆ†æ ‡ç­¾ (å­—ç¬¦ä¸²)
target_tokens = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"]

@app.on_event("startup")
async def load_model():
    global model, processor, token_ids, weights
    print(f"ğŸš€ [System] æ­£åœ¨åŠ è½½ Qwen2-VL æ¨¡å‹å¹¶æ³¨å…¥ DPO æƒé‡...")
    
    try:
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_PATH, 
            torch_dtype=torch.bfloat16, 
            device_map="auto"
        )
        model.load_adapter(LORA_PATH)
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        processor.tokenizer.padding_side = 'left' 

        # è·å– 1-10 çš„ Token ID
        token_ids = [processor.tokenizer.encode(t, add_special_tokens=False)[-1] for t in target_tokens]
        # å¯¹åº”çš„æ•°å€¼æƒé‡
        weights = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32).to(model.device)
        
        print("âœ… [System] æ¨¡å‹åŠ è½½å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ [System] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise e

@app.post("/analyze")
async def analyze_image(file: UploadFile = File(...)):
    if model is None:
        raise HTTPException(status_code=503, detail="æ¨¡å‹æœªå°±ç»ª")

    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="è¯·ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶")
    
    try:
        # 1. å¤„ç†å›¾ç‰‡
        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert("RGB")
        
        # 2. æ„é€  Prompt
        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image, "max_pixels": MAX_PIXELS},
                {"type": "text", "text": "Analyze this image's aesthetics. Briefly describe the composition and lighting, then provide a rating level from 1 to 10. Format: Analysis: [text] Rating Level: [score]"}
            ]
        }]

        text_input = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text_input], 
            images=image_inputs, 
            videos=video_inputs, 
            padding=True, 
            return_tensors="pt"
        ).to(model.device)

        # 3. æ¨ç† (å¼€å¯ output_scores)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,
                do_sample=False, 
                return_dict_in_generate=True,
                output_scores=True 
            )

        # 4. åå¤„ç†
        generated_ids = outputs.sequences[0][len(inputs.input_ids[0]):]
        generated_text = processor.decode(generated_ids, skip_special_tokens=True)
        
        # === æ ¸å¿ƒé€»è¾‘ï¼šæå– Logits åˆ†å¸ƒ ===
        final_score = 5.0
        score_distribution = {} # ç”¨äºå­˜å‚¨ "1": 0.01, "2": 0.05 ...
        rating_token_pos = -1
        
        # å®šä½ Rating æ•°å­—çš„ä½ç½®
        for pos, tid in enumerate(generated_ids):
            if tid.item() in token_ids and pos > 10: 
                rating_token_pos = pos
                break
        
        if rating_token_pos != -1:
            # A. æå– Logits
            logits = outputs.scores[rating_token_pos][0]
            relevant_logits = logits[token_ids]
            
            # B. è®¡ç®—æ¦‚ç‡ (Temperature=0.5)
            # Logits æœ¬èº«èŒƒå›´å¾ˆå¤§ä¸”æœ‰è´Ÿæ•°ï¼Œä¸é€‚åˆç›´æ¥å±•ç¤ºï¼Œå‰ç«¯é€šå¸¸éœ€è¦æ¦‚ç‡
            temperature = 0.5
            probs = F.softmax(relevant_logits.float() / temperature, dim=-1)
            
            # C. è®¡ç®—åŠ æƒåˆ†
            final_score = torch.sum(probs * weights).item()
            
            # D. [æ–°å¢] æ„å»ºè¯¦ç»†åˆ†å¸ƒå­—å…¸
            # å°† Tensor è½¬ä¸º Python List ä»¥ä¾¿ JSON åºåˆ—åŒ–
            probs_list = probs.tolist() 
            
            for i, score_label in enumerate(target_tokens):
                # å°†æ¦‚ç‡ä¿ç•™ 4 ä½å°æ•°
                score_distribution[score_label] = round(probs_list[i], 4)
        else:
            # å…œåº•ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°å­—ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒæˆ–ç©º
            for label in target_tokens:
                score_distribution[label] = 0.0
        
        return {
            "status": "success",
            "data": {
                "score": round(final_score, 4),
                "analysis": generated_text.replace("Analysis:", "").replace("Rating Level:", "").strip(),
                "distribution": score_distribution, # <--- æ–°å¢å­—æ®µ
                "raw_text": generated_text
            }
        }

    except Exception as e:
        print(f"ERROR: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=6006)


# ç¤ºä¾‹è¿”å›æ ¼å¼ï¼š
#     {
#     "status": "success",
#     "data": {
#         "score": 8.7421,
#         "analysis": "The composition is excellent...",
#         "distribution": {
#             "1": 0.0000,
#             "2": 0.0000,
#             "3": 0.0001,
#             "4": 0.0005,
#             "5": 0.0023,
#             "6": 0.0150,
#             "7": 0.1200,
#             "8": 0.6500,  <-- æ¨¡å‹æœ€å€¾å‘äº 8 åˆ†
#             "9": 0.2000,
#             "10": 0.0121
#         },
#         "raw_text": "..."
#     }
# }